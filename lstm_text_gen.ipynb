{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules imported.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Activation, Dropout, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "import random\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print('Modules imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists from the lyrics for 57650 songs. The data has been acquired from LyricsFreak through scraping. Then some very basic work has been done on removing inconvenient data: non-English lyrics, extremely short and extremely long lyrics, lyrics with non-ASCII symbols. The dataset contains 4 columns:\n",
    "\n",
    "* Artist\n",
    "* Song Name\n",
    "* Link to a webpage with the song (for reference). This is to be concatenated with http://www.lyricsfreak.com to form a real URL.\n",
    "* Lyrics of the song, unmodified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./dataset/songdata.csv', usecols=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 68056106\n",
      "Example text: look at her face, it's a wonderful face  \n",
      "and it means something special to me  \n",
      "look at the way that she smiles when she sees me  \n",
      "how lucky can one fellow be?  \n",
      "  \n",
      "she's just my kind of girl, she makes me feel fine  \n",
      "who could ever believe that she could be mine?  \n",
      "she's just my kind of girl, with\n"
     ]
    }
   ],
   "source": [
    "corpus = data['text'].str.cat(sep='\\n').lower()\n",
    "print('Corpus length:', len(corpus))\n",
    "print('Example text:', corpus[0:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncated corpus length: 1000000\n"
     ]
    }
   ],
   "source": [
    "# truncating the corpus\n",
    "# since it is going to take too long to train\n",
    "corpus = corpus[:1000000]\n",
    "print('Truncated corpus length:', len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chars: 50\n",
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '?', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# creating a character vocabulary\n",
    "chars = sorted(list(set(corpus)))\n",
    "print('Total chars:', len(chars))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating lookup dictionaries\n",
    "char_to_ind = dict((c, i) for i, c in enumerate(chars))\n",
    "ind_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 40 # the window size\n",
    "step = 3 # The steps between the windows\n",
    "sentences = []\n",
    "next_chars = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of sentences: (333320,)\n",
      "[\"look at her face, it's a wonderful face \"\n",
      " \"k at her face, it's a wonderful face  \\na\"\n",
      " \"t her face, it's a wonderful face  \\nand \" ...,\n",
      " \"t that we're less worse  \\n  \\ntears are n\"\n",
      " \"hat we're less worse  \\n  \\ntears are not \"\n",
      " \" we're less worse  \\n  \\ntears are not eno\"]\n",
      "[' ' 'n' 'i' ..., 'o' 'e' 'u']\n"
     ]
    }
   ],
   "source": [
    "# sentences as features, next chars as labels\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen]) # range from current index i for max length characters \n",
    "    next_chars.append(text[i + maxlen]) # the next character after that \n",
    "sentences = np.array(sentences)\n",
    "next_chars = np.array(next_chars)\n",
    "print('Shape of sentences:', sentences.shape)\n",
    "print(sentences)\n",
    "print(next_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integer encoded X: [192992 181993 264233 ..., 266428 157152  73624]\n",
      "Integer encoded y: [ 1 37 32 ..., 38 28 44]\n"
     ]
    }
   ],
   "source": [
    "# convert each character to categorical numbers\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded_X = label_encoder.fit_transform(sentences)\n",
    "integer_encoded_y = label_encoder.fit_transform(next_chars)\n",
    "\n",
    "print('Integer encoded X:', integer_encoded_X)\n",
    "print('Integer encoded y:', integer_encoded_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-164a893bb629>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minteger_encoded_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minteger_encoded_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minteger_encoded_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0monehot_encoded_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monehot_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minteger_encoded_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0minteger_encoded_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minteger_encoded_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minteger_encoded_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/conda/envs/lstm-text-gen/lib/python3.5/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   2017\u001b[0m         \"\"\"\n\u001b[1;32m   2018\u001b[0m         return _transform_selected(X, self._fit_transform,\n\u001b[0;32m-> 2019\u001b[0;31m                                    self.categorical_features, copy=True)\n\u001b[0m\u001b[1;32m   2020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2021\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/conda/envs/lstm-text-gen/lib/python3.5/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36m_transform_selected\u001b[0;34m(X, transform, selected, copy)\u001b[0m\n\u001b[1;32m   1810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mselected\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"all\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1812\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/conda/envs/lstm-text-gen/lib/python3.5/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36m_fit_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   2003\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_features_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactive_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2005\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/conda/envs/lstm-text-gen/lib/python3.5/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m         \u001b[0;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;31m##############################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/conda/envs/lstm-text-gen/lib/python3.5/site-packages/scipy/sparse/coo.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0mfortran\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfortran\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/conda/envs/lstm-text-gen/lib/python3.5/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1037\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__numpy_ufunc__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# one-hot encode each categorical number\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "integer_encoded_X = integer_encoded_X.reshape(len(integer_encoded_X), 1)\n",
    "onehot_encoded_X = onehot_encoder.fit_transform(integer_encoded_X)\n",
    "\n",
    "integer_encoded_y = integer_encoded_y.reshape(len(integer_encoded_y), 1)\n",
    "onehot_encoded_y = onehot_encoder.fit_transform(integer_encoded_y)\n",
    "\n",
    "\n",
    "print(onehot_encoded_X, onehot_encoded_y)\n",
    "\n",
    "X = onehot_encoded_X\n",
    "y = onehot_encoded_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one-hot encoding the input values wrapped in a separate function,\n",
    "# since the dataset length causes the memory error (on my machine) when converting all at once\n",
    "def encode(sentences, next_chars):\n",
    "    X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "    y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "    length = len(sentences)\n",
    "    index = 0\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t, char_to_ind[char]] = 1\n",
    "        y[i, char_to_ind[next_chars[i]]] = 1\n",
    "    return X, y\n",
    "\n",
    "X, y = encode(sentences, next_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 50)                6450      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 50)                0         \n",
      "=================================================================\n",
      "Total params: 98,098\n",
      "Trainable params: 98,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training checkpoints\n",
    "filepath=\"weights{epoch:02d}-{loss:.4f}.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 1/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: 2.4973Epoch 00000: loss improved from inf to 2.49725, saving model to weights00-2.4972.h5\n",
      "10000/10000 [==============================] - 3762s - loss: 2.4972  \n",
      "Epoch 2/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: 2.4876Epoch 00001: loss improved from 2.49725 to 2.48755, saving model to weights01-2.4876.h5\n",
      "10000/10000 [==============================] - 3302s - loss: 2.4876  \n",
      "Epoch 3/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: 2.4880Epoch 00002: loss did not improve\n",
      "10000/10000 [==============================] - 2940s - loss: 2.4880  \n",
      "Epoch 4/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: 2.4871Epoch 00003: loss improved from 2.48755 to 2.48713, saving model to weights03-2.4871.h5\n",
      "10000/10000 [==============================] - 2997s - loss: 2.4871  \n",
      "Epoch 5/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: 2.4881Epoch 00004: loss did not improve\n",
      "10000/10000 [==============================] - 2964s - loss: 2.4881  \n",
      "Epoch 6/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: 2.4858Epoch 00005: loss improved from 2.48713 to 2.48579, saving model to weights05-2.4858.h5\n",
      "10000/10000 [==============================] - 2964s - loss: 2.4858  \n",
      "Epoch 7/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: 2.4870Epoch 00006: loss did not improve\n",
      "10000/10000 [==============================] - 2969s - loss: 2.4870  \n",
      "Epoch 8/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: 2.4861Epoch 00007: loss did not improve\n",
      "10000/10000 [==============================] - 2970s - loss: 2.4861  \n",
      "Epoch 9/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: 2.4867Epoch 00008: loss did not improve\n",
      "10000/10000 [==============================] - 2969s - loss: 2.4867  \n",
      "Epoch 10/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: 2.4866Epoch 00009: loss did not improve\n",
      "10000/10000 [==============================] - 2976s - loss: 2.4866  \n",
      "Epoch 11/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: 2.4862Epoch 00010: loss did not improve\n",
      "10000/10000 [==============================] - 2979s - loss: 2.4862  \n",
      "Epoch 12/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: 2.4860Epoch 00011: loss did not improve\n",
      "10000/10000 [==============================] - 3010s - loss: 2.4860  \n",
      "Epoch 13/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: 2.4853Epoch 00012: loss improved from 2.48579 to 2.48532, saving model to weights12-2.4853.h5\n",
      "10000/10000 [==============================] - 3119s - loss: 2.4853  \n",
      "Epoch 14/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: 2.4854Epoch 00013: loss did not improve\n",
      "10000/10000 [==============================] - 3644s - loss: 2.4854  \n",
      "Epoch 15/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: 2.4854Epoch 00014: loss did not improve\n",
      "10000/10000 [==============================] - 3556s - loss: 2.4854  \n",
      "Epoch 16/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: 2.4840Epoch 00015: loss improved from 2.48532 to 2.48400, saving model to weights15-2.4840.h5\n",
      "10000/10000 [==============================] - 3631s - loss: 2.4840  \n",
      "Epoch 17/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: 2.4842Epoch 00016: loss did not improve\n",
      "10000/10000 [==============================] - 3586s - loss: 2.4842  \n",
      "Epoch 18/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: 2.4865Epoch 00017: loss did not improve\n",
      "10000/10000 [==============================] - 3411s - loss: 2.4865  \n",
      "Epoch 19/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: nanEpoch 00018: loss did not improve\n",
      "10000/10000 [==============================] - 3066s - loss: nan  \n",
      "Epoch 20/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: nanEpoch 00019: loss did not improve\n",
      "10000/10000 [==============================] - 3509s - loss: nan  \n",
      "Epoch 21/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: nanEpoch 00020: loss did not improve\n",
      "10000/10000 [==============================] - 3015s - loss: nan  \n",
      "Epoch 22/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: nanEpoch 00021: loss did not improve\n",
      "10000/10000 [==============================] - 3007s - loss: nan  \n",
      "Epoch 23/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: nanEpoch 00022: loss did not improve\n",
      "10000/10000 [==============================] - 2993s - loss: nan  \n",
      "Epoch 24/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: nanEpoch 00023: loss did not improve\n",
      "10000/10000 [==============================] - 2989s - loss: nan  \n",
      "Epoch 25/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: nanEpoch 00024: loss did not improve\n",
      "10000/10000 [==============================] - 2992s - loss: nan  \n",
      "Epoch 26/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: nanEpoch 00025: loss did not improve\n",
      "10000/10000 [==============================] - 3001s - loss: nan  \n",
      "Epoch 27/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: nanEpoch 00026: loss did not improve\n",
      "10000/10000 [==============================] - 3011s - loss: nan  \n",
      "Epoch 28/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: nanEpoch 00027: loss did not improve\n",
      "10000/10000 [==============================] - 3014s - loss: nan  \n",
      "Epoch 29/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: nanEpoch 00028: loss did not improve\n",
      "10000/10000 [==============================] - 3017s - loss: nan  \n",
      "Epoch 30/30\n",
      " 9999/10000 [============================>.] - ETA: 0s - loss: nanEpoch 00029: loss did not improve\n",
      "10000/10000 [==============================] - 3015s - loss: nan  \n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "batch_size = 100\n",
    "\n",
    "trained = model.fit(X, y,batch_size=100, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction per char in vocabulary [[  1.14192622e-09   2.61941582e-01   1.22791299e-09   1.11195997e-09\n",
      "    9.89448745e-10   1.04919839e-09   1.12323861e-09   1.25236310e-09\n",
      "    1.16939869e-09   1.19848964e-09   1.23993693e-09   1.14135135e-09\n",
      "    1.10743481e-09   1.24895194e-09   1.09906406e-09   1.11621845e-09\n",
      "    1.14694454e-09   1.25958333e-09   1.13789311e-09   1.07535858e-09\n",
      "    1.28547040e-09   1.16846910e-09   1.20295551e-09   1.19953469e-09\n",
      "    4.75156344e-02   2.43664589e-02   2.40089260e-02   1.07945619e-09\n",
      "    1.66302472e-01   1.05106324e-09   1.18246357e-09   2.38009505e-02\n",
      "    4.71939221e-02   1.08539600e-09   2.40857508e-02   4.77083512e-02\n",
      "    7.14605674e-02   4.73378152e-02   7.16472492e-02   1.19791133e-09\n",
      "    1.16055965e-09   1.05425957e-09   7.10829794e-02   7.15472549e-02\n",
      "    1.10959775e-09   1.15319188e-09   1.19163091e-09   1.24365329e-09\n",
      "    1.19769439e-09   1.13973075e-09]]\n"
     ]
    }
   ],
   "source": [
    "# example test phrase\n",
    "sentence = 'dance all night'\n",
    "\n",
    "# construct a vector from an example phrase\n",
    "x = np.zeros((1, maxlen, len(chars)))\n",
    "for i, char in enumerate(sentence):\n",
    "    x[0, i, char_to_ind[char]] = 1.\n",
    "    \n",
    "# load trained weights\n",
    "model = load_model('weights01-2.3384.h5')\n",
    "\n",
    "\n",
    "print('Predictions per char in vocabulary', model.predict(x, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: adding randomness for choosing predicted characters\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature:  1\n",
      "dance all nights ho eneeakmecee  st  mteeb n me neonooe   it  albc mns oe amhce  eosom m  asce b  eeetnee sse henmel s   noembetlsineobamsiomelennt te oh l   ekeot  ea a    o io os eo  kb tioe e la ecm  elk elmi be  oo k n tbinste l ososnenetel ic am ts teo esi  iml oo isei st eebiaeheleitienosson  msn nent eoeee ae o tiktaem nlb lkekobeec cecb i tlemsom ne nl c o el  monce  m eten mtsalmo ktbi se ettso  bse  n  oem eebtsmnch  ecekoekiteol eoncec t   teeetbt tmtee tte ee teac   balcblka  snetooiam haotteeile  \n"
     ]
    }
   ],
   "source": [
    "# temperature defines randomness rate for using predictions\n",
    "temperature = 1\n",
    "print('Temperature: ', temperature)\n",
    "\n",
    "generated = ''\n",
    "original = sentence\n",
    "window = sentence\n",
    "\n",
    "# predicting next 500 chars\n",
    "for i in range(500):\n",
    "    x = np.zeros((1, maxlen, len(chars)))\n",
    "    for t, char in enumerate(window):\n",
    "        x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "    preds = model.predict(x, verbose=0)[0]\n",
    "    next_index = sample(preds, temperature)\n",
    "    next_char = indices_char[next_index]\n",
    "\n",
    "    generated += next_char\n",
    "    window = window[1:] + next_char\n",
    "\n",
    "print(original + generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
