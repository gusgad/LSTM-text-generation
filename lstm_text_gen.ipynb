{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules imported.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Activation, Dropout, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "import random\n",
    "\n",
    "# to later delete \"optimizer_weights\" part of the model weights\n",
    "# due to errors that occur when training on gpu and testing on cpu\n",
    "import h5py\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print('Modules imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists from the lyrics for 57650 songs. The data has been acquired from LyricsFreak through scraping. Then some very basic work has been done on removing inconvenient data: non-English lyrics, extremely short and extremely long lyrics, lyrics with non-ASCII symbols. The dataset contains 4 columns:\n",
    "\n",
    "* Artist\n",
    "* Song Name\n",
    "* Link to a webpage with the song (for reference). This is to be concatenated with http://www.lyricsfreak.com to form a real URL.\n",
    "* Lyrics of the song, unmodified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./dataset/songdata.csv', usecols=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 68056106\n",
      "Example text: look at her face, it's a wonderful face  \n",
      "and it means something special to me  \n",
      "look at the way that she smiles when she sees me  \n",
      "how lucky can one fellow be?  \n",
      "  \n",
      "she's just my kind of girl, she makes me feel fine  \n",
      "who could ever believe that she could be mine?  \n",
      "she's just my kind of girl, with\n"
     ]
    }
   ],
   "source": [
    "corpus = data['text'].str.cat(sep='\\n').lower()\n",
    "print('Corpus length:', len(corpus))\n",
    "print('Example text:', corpus[0:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncated corpus length: 1000000\n"
     ]
    }
   ],
   "source": [
    "# truncating the corpus\n",
    "# since it is going to take too long to train\n",
    "corpus = corpus[:1000000]\n",
    "print('Truncated corpus length:', len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chars: 50\n",
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '?', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# creating a character vocabulary\n",
    "chars = sorted(list(set(corpus)))\n",
    "print('Total chars:', len(chars))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating lookup dictionaries\n",
    "char_to_ind = dict((c, i) for i, c in enumerate(chars))\n",
    "ind_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 40 # the window size\n",
    "step = 3 # step of the window\n",
    "sentences = []\n",
    "next_chars = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of sentences: (333320,)\n",
      "[\"look at her face, it's a wonderful face \"\n",
      " \"k at her face, it's a wonderful face  \\na\"\n",
      " \"t her face, it's a wonderful face  \\nand \" ...,\n",
      " \"t that we're less worse  \\n  \\ntears are n\"\n",
      " \"hat we're less worse  \\n  \\ntears are not \"\n",
      " \" we're less worse  \\n  \\ntears are not eno\"]\n",
      "[' ' 'n' 'i' ..., 'o' 'e' 'u']\n"
     ]
    }
   ],
   "source": [
    "# sentences as features, next chars as labels\n",
    "for i in range(0, len(corpus) - maxlen, step):\n",
    "    sentences.append(corpus[i: i + maxlen]) # range from current index i for max length characters \n",
    "    next_chars.append(corpus[i + maxlen]) # the next character after that \n",
    "sentences = np.array(sentences)\n",
    "next_chars = np.array(next_chars)\n",
    "print('Shape of sentences:', sentences.shape)\n",
    "print(sentences)\n",
    "print(next_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integer encoded X: [192992 181993 264233 ..., 266428 157152  73624]\n",
      "Integer encoded y: [ 1 37 32 ..., 38 28 44]\n"
     ]
    }
   ],
   "source": [
    "# convert each character to categorical numbers\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(next_chars)\n",
    "\n",
    "print('Integer encoded y:', integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one-hot encode each categorical number\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "integer_encoded = integer_encoded_y.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "print(onehot_encoded)\n",
    "\n",
    "y = onehot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding the input values wrapped in a separate function,\n",
    "# since the dataset length causes the memory error (on my machine) when converting all at once\n",
    "def encode(sentences, next_chars):\n",
    "    X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "    y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "    length = len(sentences)\n",
    "    index = 0\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t, char_to_ind[char]] = 1\n",
    "        y[i, char_to_ind[next_chars[i]]] = 1\n",
    "    return X, y\n",
    "\n",
    "X, y = encode(sentences, next_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                6450      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 50)                0         \n",
      "=================================================================\n",
      "Total params: 98,098\n",
      "Trainable params: 98,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training checkpoints\n",
    "filepath=\"weights{epoch:02d}-{loss:.4f}.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 2.1995Epoch 00000: loss improved from inf to 2.19947, saving model to weights00-2.1995.h5\n",
      "333320/333320 [==============================] - 163s - loss: 2.1995   \n",
      "Epoch 2/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.8032Epoch 00001: loss improved from 2.19947 to 1.80316, saving model to weights01-1.8032.h5\n",
      "333320/333320 [==============================] - 151s - loss: 1.8032   \n",
      "Epoch 3/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.6684Epoch 00002: loss improved from 1.80316 to 1.66841, saving model to weights02-1.6684.h5\n",
      "333320/333320 [==============================] - 151s - loss: 1.6684   \n",
      "Epoch 4/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.5845Epoch 00003: loss improved from 1.66841 to 1.58450, saving model to weights03-1.5845.h5\n",
      "333320/333320 [==============================] - 149s - loss: 1.5845   \n",
      "Epoch 5/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.5242Epoch 00004: loss improved from 1.58450 to 1.52414, saving model to weights04-1.5241.h5\n",
      "333320/333320 [==============================] - 149s - loss: 1.5241   \n",
      "Epoch 6/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.4780Epoch 00005: loss improved from 1.52414 to 1.47796, saving model to weights05-1.4780.h5\n",
      "333320/333320 [==============================] - 148s - loss: 1.4780   \n",
      "Epoch 7/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.4408Epoch 00006: loss improved from 1.47796 to 1.44082, saving model to weights06-1.4408.h5\n",
      "333320/333320 [==============================] - 148s - loss: 1.4408   \n",
      "Epoch 8/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.4091Epoch 00007: loss improved from 1.44082 to 1.40910, saving model to weights07-1.4091.h5\n",
      "333320/333320 [==============================] - 148s - loss: 1.4091   \n",
      "Epoch 9/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.3829Epoch 00008: loss improved from 1.40910 to 1.38295, saving model to weights08-1.3829.h5\n",
      "333320/333320 [==============================] - 148s - loss: 1.3829   \n",
      "Epoch 10/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.3605Epoch 00009: loss improved from 1.38295 to 1.36045, saving model to weights09-1.3605.h5\n",
      "333320/333320 [==============================] - 148s - loss: 1.3605   \n",
      "Epoch 11/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.3393Epoch 00010: loss improved from 1.36045 to 1.33930, saving model to weights10-1.3393.h5\n",
      "333320/333320 [==============================] - 148s - loss: 1.3393   \n",
      "Epoch 12/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.3214Epoch 00011: loss improved from 1.33930 to 1.32136, saving model to weights11-1.3214.h5\n",
      "333320/333320 [==============================] - 148s - loss: 1.3214   \n",
      "Epoch 13/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.3050Epoch 00012: loss improved from 1.32136 to 1.30503, saving model to weights12-1.3050.h5\n",
      "333320/333320 [==============================] - 148s - loss: 1.3050   \n",
      "Epoch 14/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.2902Epoch 00013: loss improved from 1.30503 to 1.29025, saving model to weights13-1.2903.h5\n",
      "333320/333320 [==============================] - 148s - loss: 1.2903   \n",
      "Epoch 15/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.2766Epoch 00014: loss improved from 1.29025 to 1.27663, saving model to weights14-1.2766.h5\n",
      "333320/333320 [==============================] - 148s - loss: 1.2766   \n",
      "Epoch 16/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.2646Epoch 00015: loss improved from 1.27663 to 1.26462, saving model to weights15-1.2646.h5\n",
      "333320/333320 [==============================] - 148s - loss: 1.2646   \n",
      "Epoch 17/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.2526Epoch 00016: loss improved from 1.26462 to 1.25264, saving model to weights16-1.2526.h5\n",
      "333320/333320 [==============================] - 148s - loss: 1.2526   \n",
      "Epoch 18/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.2420Epoch 00017: loss improved from 1.25264 to 1.24197, saving model to weights17-1.2420.h5\n",
      "333320/333320 [==============================] - 148s - loss: 1.2420   \n",
      "Epoch 19/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.2311Epoch 00018: loss improved from 1.24197 to 1.23111, saving model to weights18-1.2311.h5\n",
      "333320/333320 [==============================] - 148s - loss: 1.2311   \n",
      "Epoch 20/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.2220Epoch 00019: loss improved from 1.23111 to 1.22202, saving model to weights19-1.2220.h5\n",
      "333320/333320 [==============================] - 148s - loss: 1.2220   \n",
      "Epoch 21/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.2136Epoch 00020: loss improved from 1.22202 to 1.21358, saving model to weights20-1.2136.h5\n",
      "333320/333320 [==============================] - 149s - loss: 1.2136   \n",
      "Epoch 22/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.2054Epoch 00021: loss improved from 1.21358 to 1.20540, saving model to weights21-1.2054.h5\n",
      "333320/333320 [==============================] - 148s - loss: 1.2054   \n",
      "Epoch 23/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.1979Epoch 00022: loss improved from 1.20540 to 1.19790, saving model to weights22-1.1979.h5\n",
      "333320/333320 [==============================] - 149s - loss: 1.1979   \n",
      "Epoch 24/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.1902Epoch 00023: loss improved from 1.19790 to 1.19020, saving model to weights23-1.1902.h5\n",
      "333320/333320 [==============================] - 148s - loss: 1.1902   \n",
      "Epoch 25/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.1829Epoch 00024: loss improved from 1.19020 to 1.18288, saving model to weights24-1.1829.h5\n",
      "333320/333320 [==============================] - 148s - loss: 1.1829   \n",
      "Epoch 26/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.1766Epoch 00025: loss improved from 1.18288 to 1.17660, saving model to weights25-1.1766.h5\n",
      "333320/333320 [==============================] - 149s - loss: 1.1766   \n",
      "Epoch 27/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.1707Epoch 00026: loss improved from 1.17660 to 1.17073, saving model to weights26-1.1707.h5\n",
      "333320/333320 [==============================] - 149s - loss: 1.1707   \n",
      "Epoch 28/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.1649Epoch 00027: loss improved from 1.17073 to 1.16486, saving model to weights27-1.1649.h5\n",
      "333320/333320 [==============================] - 149s - loss: 1.1649   \n",
      "Epoch 29/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.1591Epoch 00028: loss improved from 1.16486 to 1.15906, saving model to weights28-1.1591.h5\n",
      "333320/333320 [==============================] - 149s - loss: 1.1591   \n",
      "Epoch 30/30\n",
      "333312/333320 [============================>.] - ETA: 0s - loss: 1.1538Epoch 00029: loss improved from 1.15906 to 1.15377, saving model to weights29-1.1538.h5\n",
      "333320/333320 [==============================] - 149s - loss: 1.1538   \n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "batch_size = 128\n",
    "\n",
    "trained = model.fit(X, y, batch_size, epochs=30, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: run this if the model is trained on gpu but being tested on cpu\n",
    "# RUN ONCE FOR A FILE\n",
    "f = h5py.File('weights29-1.1538.h5', 'r+')\n",
    "del f['optimizer_weights']\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence length: 40\n",
      "Predictions per char in vocabulary [[  7.85644632e-04   6.48357809e-01   2.39531408e-04   6.93314723e-05\n",
      "    1.92984089e-03   3.50722723e-04   2.05814256e-03   5.79579026e-02\n",
      "    2.30197984e-04   2.02430710e-02   3.32634727e-07   4.05892625e-10\n",
      "    1.07458886e-08   9.59632089e-08   3.98085618e-08   6.00360606e-09\n",
      "    2.51268867e-11   7.56856397e-14   6.72226077e-12   5.60327429e-08\n",
      "    8.10267807e-07   3.99530778e-04   1.47456335e-11   1.24109029e-05\n",
      "    1.28299434e-04   2.41462010e-04   3.04026980e-05   1.62235265e-05\n",
      "    2.84606474e-04   2.40745176e-05   2.13956610e-05   5.40698133e-03\n",
      "    4.33711568e-04   4.03412469e-06   5.74045345e-08   7.95448082e-04\n",
      "    3.43134016e-04   5.62815367e-05   8.08755503e-05   7.52190954e-06\n",
      "    1.85356603e-05   2.52930564e-04   2.58004218e-01   2.00749695e-04\n",
      "    2.36398409e-04   3.09081059e-07   3.25245819e-05   5.33050120e-08\n",
      "    7.44019926e-04   3.30167666e-07]]\n"
     ]
    }
   ],
   "source": [
    "# example test phrase\n",
    "sentence = 'Gabriele loves dancing\\nall day and night'\n",
    "sentence = sentence.lower()\n",
    "print('Sentence length:', len(sentence))\n",
    "\n",
    "# construct a vector from an example phrase\n",
    "x = np.zeros((1, maxlen, len(chars)))\n",
    "for i, char in enumerate(sentence):\n",
    "    x[0, i, char_to_ind[char]] = 1.\n",
    "    \n",
    "# load trained weights\n",
    "model = load_model('weights29-1.1538.h5')\n",
    "\n",
    "\n",
    "print('Predictions per char in vocabulary', model.predict(x, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: adding randomness for choosing predicted characters\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature:  0.8\n",
      "gabriele loves dancing\n",
      "all day and night  \n",
      "and when i feel the seess  \n",
      "we love yoursed you will  \n",
      "i have to things it will be a molurons day  \n",
      "well i want you to let you  \n",
      "you could tee  \n",
      "listen you through the grows  \n",
      "smiting the mornisy and she's so here around  \n",
      "and fallin' alwight  \n",
      "  \n",
      "over slowing buy sewors  \n",
      "your look why,  \n",
      "i'm living in your eyes  \n",
      "that i needder that we said  \n",
      "gonna'd like inforce  \n",
      "  \n",
      "hey need a mine to from the butle of the corner  \n",
      "it's tree  \n",
      "but you dring a frovieve  \n",
      "  \n",
      "i'm living in the morning as it \n"
     ]
    }
   ],
   "source": [
    "# temperature defines randomness rate for using predictions\n",
    "temperature = 0.8\n",
    "print('Temperature: ', temperature)\n",
    "\n",
    "generated = ''\n",
    "original = sentence\n",
    "window = sentence\n",
    "\n",
    "# predicting next 500 chars\n",
    "for i in range(500):\n",
    "    x = np.zeros((1, maxlen, len(chars)))\n",
    "    for t, char in enumerate(window):\n",
    "        x[0, t, char_to_ind[char]] = 1.\n",
    "\n",
    "    preds = model.predict(x, verbose=0)[0]\n",
    "    next_index = sample(preds, temperature)\n",
    "    next_char = ind_to_char[next_index]\n",
    "\n",
    "    generated += next_char\n",
    "    window = window[1:] + next_char\n",
    "\n",
    "print(original + generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
